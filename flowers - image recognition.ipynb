{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# data source: https://www.kaggle.com/alxmamaev/flowers-recognition\n",
    "\n",
    "%reset -f\n",
    "\n",
    "from keras.models import Sequential # to initialize CNN as a sequence of layers\n",
    "from keras.layers import Convolution2D # for convolutional operations\n",
    "from keras.layers import MaxPooling2D # for pooling operations\n",
    "from keras.layers import Flatten # to flatten stacked feature maps into input layer\n",
    "from keras.layers import Dense # to build fully-connected layers in a traditional neural network\n",
    "from keras.layers import Dropout # to build a dropout layer(s); helps prevent overfitting\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### part 1: building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 1: add a convolution layer to the CNN\n",
    "classifier.add(Convolution2D(filters=32, # 32 filters/kernels\n",
    "                             kernel_size=(3,3), # each kernel has a 3x3 receptive field\n",
    "                             padding='same', # spatial dimensions of input image and feature map are the same\n",
    "                             input_shape=(64,64,3), # each input image has width=height=64, and 3 color channels\n",
    "                             strides=(1,1), # stride 1 pixel at a time, along the width and height\n",
    "                             activation='relu')) # apply ReLU activation function to the conv layer element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 2: add a pooling layer to the CNN\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), # pooling kernel has a 2x2 receptive field\n",
    "                            strides=(2,2))) # stride 2 pixels at a time, along the width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 3: repeat step 1 and 2 for a deep CNN\n",
    "classifier.add(Convolution2D(filters=32, # 32 filters\n",
    "                             kernel_size=(3,3), # each kernel has a 3x3 receptive field\n",
    "                             padding='same', # spatial dimensions of input image and feature map are the same\n",
    "                             strides=(1,1), # stride 1 pixel at a time, along the width and height\n",
    "                             activation='relu')) # apply ReLU activation function to the conv layer element-wise\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), # pooling kernel has a 2x2 receptive field\n",
    "                            strides=(2,2))) # stride 2 pixels at a time, along the width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 4: flatten the pooled feature maps into a vector of neurons (to be used as an input layer in a classic NN)\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 5: fully connect the input layer to a hidden layer and output layer (classic NN)\n",
    "classifier.add(Dropout(0.5)) # dropout layer (with 50% probability of shutting down any given neuron) prevents overfitting\n",
    "\n",
    "classifier.add(Dense(units=128, # 128 neurons in the hidden layer\n",
    "                     activation='relu'))\n",
    "\n",
    "classifier.add(Dropout(0.5)) # dropout layer (with 50% probability of shutting down any given neuron) prevents overfitting\n",
    "\n",
    "classifier.add(Dense(units=5, # if classification problem was binary, then 1 unit\n",
    "                     activation='softmax')) # if classification problem was binary, we'd use sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile the CNN\n",
    "classifier.compile(optimizer='adam', # stochastic gradient descent algorithm (to optimize the weights)\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy']) # alternative: crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### part 2: fitting the CNN to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4073 images belonging to 5 classes.\n",
      "Found 250 images belonging to 5 classes.\n",
      "Epoch 1/3\n",
      "4073/4073 [==============================] - 3959s 972ms/step - loss: 0.7397 - acc: 0.7124 - val_loss: 0.7446 - val_acc: 0.7560\n",
      "Epoch 2/3\n",
      "4073/4073 [==============================] - 3438s 844ms/step - loss: 0.4400 - acc: 0.8345 - val_loss: 0.8155 - val_acc: 0.7560\n",
      "Epoch 3/3\n",
      "4073/4073 [==============================] - 3694s 907ms/step - loss: 0.3372 - acc: 0.8728 - val_loss: 0.9146 - val_acc: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c99a88240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### prevent overfitting by augmenting the number of images with transformed (e.g. zoomed, sheared) versions of those images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "os.chdir('D:\\data')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('training_set',\n",
    "                                                 target_size=(64,64), # all images will be resized to 64x64 pixels\n",
    "                                                 batch_size=32, # weights are updated every time 32 images have been fed into the CNN\n",
    "                                                 class_mode='categorical') # multiclass classification\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('test_set',\n",
    "                                            target_size=(64,64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch=4073, # number of images in the training set\n",
    "                         epochs=3, # number of times you want to feed all the training images into the CNN\n",
    "                         validation_data=test_set,\n",
    "                         validation_steps=250) # number of images in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
